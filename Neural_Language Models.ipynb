{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfWYm/OTf0ITE62zo3VvaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debbiekj/Neural-Language-Models/blob/main/Neural_Language%C2%A0Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word2Vec**"
      ],
      "metadata": {
        "id": "ARRm9J7dBgtm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCqMOPNmXxvF",
        "outputId": "44620822-a07a-4a2a-b719-c8d7dd7b21ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "r4KZGIWyYR_K"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [[\"I\", \"Like\", \"You\"], [\"You\", \"Are\", \"Amazing\"]]"
      ],
      "metadata": {
        "id": "eKzdqrEdbbIB"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min_count indicates word's minimum frequency.\n",
        "model = Word2Vec(sentences, min_count=1)"
      ],
      "metadata": {
        "id": "rfdIaecAbhMG"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = model.wv[\"You\"]\n",
        "print(vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ73AFlRbyzO",
        "outputId": "bd5e75b0-1dbd-4edd-9cd2-d48d00ea379d"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
            " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
            " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
            " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
            "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
            "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
            "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
            " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
            "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
            "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
            " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
            " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
            "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
            " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
            "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
            " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
            " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
            " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
            " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
            "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
            " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
            " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
            " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
            "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
            " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consine** **similarity** **between** **the** **sentences**"
      ],
      "metadata": {
        "id": "OEHxg1fhps5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the word vectors for each word in the sentences\n",
        "sentence_vectors = []\n",
        "for sentence in sentences:\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if vectors:\n",
        "        sentence_vectors.append(np.mean(vectors, axis=0))\n",
        "    else:\n",
        "        # In case none of the words in the sentence are in the model's vocabulary\n",
        "        sentence_vectors.append(np.zeros(model.vector_size))\n",
        "\n",
        "# Calculate the cosine similarity between the sentences\n",
        "similarity = cosine_similarity([sentence_vectors[0]], [sentence_vectors[1]])\n",
        "\n",
        "print(\"Cosine similarity:\", similarity[0][0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiWHl4Wch6Z1",
        "outputId": "30fc9736-f3de-4693-fcd9-a2d5b00a0def"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: 0.3679116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM-based language **model**"
      ],
      "metadata": {
        "id": "eTivZ86S4aNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "tYPzUEXhpd7u"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    \"I love donuts\",\n",
        "    \"Mangoes are my favourite fruit\",\n",
        "    \"I exercise everyday\"]"
      ],
      "metadata": {
        "id": "pBR8eWjVuCO7"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "A4M7VpPCubyr"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "output_words = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        input_sequences.append(sequence[:i])\n",
        "        output_words.append(sequence[i])\n"
      ],
      "metadata": {
        "id": "zgylb5fwupKL"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length)\n"
      ],
      "metadata": {
        "id": "HtE9s9-Uuxwc"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(padded_sequences)\n",
        "y = to_categorical(output_words, num_classes=vocab_size)\n",
        "\n",
        "\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n"
      ],
      "metadata": {
        "id": "LYud8h5zvX9M"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_sequence_length, 1), activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIkxSxqaw9lO",
        "outputId": "9369c7a0-7411-4b2e-d269-9d7654fb961e"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3945 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3778 - accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.3613 - accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3451 - accuracy: 0.1250\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3290 - accuracy: 0.1250\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.3131 - accuracy: 0.1250\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.2972 - accuracy: 0.1250\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.2814 - accuracy: 0.1250\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.2656 - accuracy: 0.1250\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2497 - accuracy: 0.1250\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2337 - accuracy: 0.1250\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.2176 - accuracy: 0.1250\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2013 - accuracy: 0.1250\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1847 - accuracy: 0.1250\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1679 - accuracy: 0.1250\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1509 - accuracy: 0.1250\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1336 - accuracy: 0.1250\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1163 - accuracy: 0.1250\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0988 - accuracy: 0.1250\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0814 - accuracy: 0.1250\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0641 - accuracy: 0.1250\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0470 - accuracy: 0.1250\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0302 - accuracy: 0.1250\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0139 - accuracy: 0.1250\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9980 - accuracy: 0.1250\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9827 - accuracy: 0.1250\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9679 - accuracy: 0.1250\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9535 - accuracy: 0.1250\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9394 - accuracy: 0.1250\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9254 - accuracy: 0.1250\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9112 - accuracy: 0.1250\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8968 - accuracy: 0.1250\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8821 - accuracy: 0.1250\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8669 - accuracy: 0.1250\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8513 - accuracy: 0.1250\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8354 - accuracy: 0.1250\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8190 - accuracy: 0.1250\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8023 - accuracy: 0.1250\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7851 - accuracy: 0.1250\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7675 - accuracy: 0.1250\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7495 - accuracy: 0.2500\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7312 - accuracy: 0.3750\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7125 - accuracy: 0.3750\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6937 - accuracy: 0.3750\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6747 - accuracy: 0.3750\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6557 - accuracy: 0.3750\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6363 - accuracy: 0.3750\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6165 - accuracy: 0.3750\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5963 - accuracy: 0.3750\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5758 - accuracy: 0.3750\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5552 - accuracy: 0.3750\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5347 - accuracy: 0.3750\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5138 - accuracy: 0.3750\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4928 - accuracy: 0.3750\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4718 - accuracy: 0.3750\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4509 - accuracy: 0.3750\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4302 - accuracy: 0.3750\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4095 - accuracy: 0.5000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3889 - accuracy: 0.5000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.3682 - accuracy: 0.5000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3475 - accuracy: 0.5000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3267 - accuracy: 0.5000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3055 - accuracy: 0.5000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.2843 - accuracy: 0.5000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2628 - accuracy: 0.5000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2412 - accuracy: 0.5000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2192 - accuracy: 0.5000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.1969 - accuracy: 0.5000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.1744 - accuracy: 0.6250\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.1517 - accuracy: 0.6250\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.1287 - accuracy: 0.6250\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.1052 - accuracy: 0.6250\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0816 - accuracy: 0.6250\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0575 - accuracy: 0.6250\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0332 - accuracy: 0.6250\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0087 - accuracy: 0.6250\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9842 - accuracy: 0.6250\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9595 - accuracy: 0.6250\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9354 - accuracy: 0.6250\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9117 - accuracy: 0.7500\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8880 - accuracy: 0.7500\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8645 - accuracy: 0.7500\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8415 - accuracy: 0.7500\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8188 - accuracy: 0.8750\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7966 - accuracy: 0.8750\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7751 - accuracy: 0.8750\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7544 - accuracy: 0.8750\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7347 - accuracy: 0.8750\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7157 - accuracy: 0.8750\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6976 - accuracy: 0.8750\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6803 - accuracy: 0.8750\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6637 - accuracy: 0.8750\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6478 - accuracy: 0.8750\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - accuracy: 0.8750\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6177 - accuracy: 0.8750\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6031 - accuracy: 0.8750\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5888 - accuracy: 0.8750\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5749 - accuracy: 0.8750\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5612 - accuracy: 0.8750\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5475 - accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79cb0d953460>"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I love\"\n",
        "seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "padded_seed_sequence = pad_sequences([seed_sequence], maxlen=max_sequence_length)\n",
        "predicted_probabilities = model.predict(padded_seed_sequence, verbose=0)\n",
        "predicted_word_index = np.argmax(predicted_probabilities)\n",
        "predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "print(\"Generated Text:\", seed_text + \" \" + predicted_word)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtU2-yJJyIhl",
        "outputId": "b3114362-48fb-48ae-8529-9a9aacea1a9d"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: I love donuts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU-based language **model**"
      ],
      "metadata": {
        "id": "PQCxYgJU4I75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GRU, Dense\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "9Yymyj0S4ITb"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_sentences = [\n",
        "    \"I love birds\",\n",
        "    \"They are amazing\",\n",
        "\n",
        " ]\n",
        "\n",
        "# Convert sentences to sequences of integers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(my_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(my_sentences)\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Prepare input and output data\n",
        "X = padded_sequences[:, :-1]\n",
        "y = to_categorical(padded_sequences[:, -1], num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n"
      ],
      "metadata": {
        "id": "byd8L2LR4sVs"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length - 1))\n",
        "model.add(GRU(100))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.fit(X, y, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku-iFb7l5Vx5",
        "outputId": "18d8e7a4-d1c9-44cd-afc5-4e323d21981f"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.9609\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9340\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9073\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8804\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8533\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8257\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7975\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7685\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7384\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7071\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.6744\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6402\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6042\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5664\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5265\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4846\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4403\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3938\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3448\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2932\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2392\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1827\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1237\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0624\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9990\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9337\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8670\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7992\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7309\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6629\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5957\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5303\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4674\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4079\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3524\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3014\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2555\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2146\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1790\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1483\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1222\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1004\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0822\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0672\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0550\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0451\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0371\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0306\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0253\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0211\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79cb085455a0>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I love\"\n",
        "seed_sequence = tokenizer.texts_to_sequences([seed_text])[0][-max_sequence_length+1:]\n",
        "padded_seed_sequence = pad_sequences([seed_sequence], maxlen=max_sequence_length-1)\n",
        "predicted_probabilities = model.predict(padded_seed_sequence, verbose=0)\n",
        "predicted_word_index = np.argmax(predicted_probabilities)\n",
        "predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "generated_text = seed_text + \" \" + predicted_word\n",
        "print(\"Generated Text:\", generated_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XghNjoH8ZFZ",
        "outputId": "52e0034f-9436-4ccc-d6e9-c2ce38743217"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: I love birds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sequence-to-Sequence** **Model**"
      ],
      "metadata": {
        "id": "a6D8Wr2BGWfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dense\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "5BC8gMFJIdi2"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model parameters\n",
        "vocab_size = 10000\n",
        "max_sequence_length = 50\n",
        "embedding_dim = 256\n",
        "num_heads = 8\n",
        "hidden_dim = 512\n",
        "\n",
        "# Define the encoder input\n",
        "encoder_input = Input(shape=(max_sequence_length,))\n",
        "\n",
        "# Embedding layer\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_input)\n",
        "\n",
        "# Multi-head self-attention layer\n",
        "encoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(encoder_embedding, encoder_embedding)\n",
        "encoder_output = tf.keras.layers.GlobalAveragePooling1D()(encoder_attention)\n",
        "\n",
        "# Feed-forward network\n",
        "encoder_output = Dense(hidden_dim, activation='relu')(encoder_output)\n",
        "\n",
        "# Define the decoder input\n",
        "decoder_input = Input(shape=(max_sequence_length,))\n",
        "\n",
        "# Embedding layer\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_input)\n",
        "\n",
        "# Multi-head self-attention layer\n",
        "decoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(decoder_embedding, decoder_embedding)\n",
        "decoder_output = tf.keras.layers.GlobalAveragePooling1D()(decoder_attention)\n",
        "\n",
        "# Feed-forward network\n",
        "decoder_output = Dense(hidden_dim, activation='relu')(decoder_output)\n",
        "\n",
        "# Define the output layer\n",
        "output = Dense(vocab_size, activation='softmax')(decoder_output)"
      ],
      "metadata": {
        "id": "B5S8GacvIhK-"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = Model(inputs=[encoder_input, decoder_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fXNLd_PdIpbm"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOZXq64yJyMM",
        "outputId": "94569486-9b67-4508-b6f3-863538410e4e"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_13 (Embedding)       (None, 50, 256)      2560000     ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 50, 256)     2103552     ['embedding_13[0][0]',           \n",
            " eadAttention)                                                    'embedding_13[0][0]']           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_4 (Gl  (None, 256)         0           ['multi_head_attention_4[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 512)          131584      ['global_average_pooling1d_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 10000)        5130000     ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,925,136\n",
            "Trainable params: 9,925,136\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}